<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>架构剖析 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</title><link>vesoft-inc.github.io/nebula-website/cn/tags/%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</link><description>Recent content in 架构剖析 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 04 Feb 2020 00:00:00 +0000</lastBuildDate><atom:link href="vesoft-inc.github.io/nebula-website/cn/tags/%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/index.xml" rel="self" type="application/rss+xml"/><item><title>图数据库设计实践 | 存储服务的负载均衡和数据迁移</title><link>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</guid><description>在文章《Nebula 架构剖析系列（一）图数据库的存储设计》中，我们提过分布式图存储的管理由 Meta Service 来统一调度，它记录了所有 partition 的分布情况，以及当前机器的状态。当 DBA 增减机器时，只需要通过 console 输入相应的指令，Meta Service 便能够生成整个 Balance 计划并执行。而之所以没有采用完全自动 Balance 的方式，主要是为了减少数据搬迁对于线上服务的影响，Balance 的时机由用户自己控制。
在本文中我们将着重讲解在存储层如何实现数据和服务的负载平衡。
简单回顾一下，Nebula Graph 的服务可分为 graph，storage，meta。本文主要描述对于存储层（storage）的数据和服务的 balance。这些都是通过 Balance 命令来实现的：Balance 命令有两种，一种需要迁移数据，命令为 BALANCE DATA ；另一种不需要迁移数据，只改变 partition 的 raft-leader 分布（负载均衡），命令为 BALANCE LEADER 。
本文目录 Balance 机制浅析 集群数据迁移 Step 1：准备工作 Step 1.1 查看现有集群状态 Step 1.2 创建图空间 Step 2 加入新实例 Step 3 迁移数据 Step 4 假如要中途停止 balance data Step 5 查看数据迁移结果 Step 6 Balance leader 批量缩容 示例数据迁移 Balance 机制浅析 在图数据库 Nebula Graph 中， Balance 主要用来 balance leader 和 partition，只涉及 leader 和 partition 在机器之间转移，不会增加或者减少 leader 和 partition 的数量。</description></item><item><title>Nebula 架构剖析系列（二）图数据库的查询引擎设计</title><link>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-query-engine-overview/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-query-engine-overview/</guid><description>摘要 上文（存储篇）说到数据库重要的两部分为存储和计算，本篇内容为你解读图数据库 Nebula 在查询引擎 Query Engine 方面的设计实践。
在 Nebula 中，Query Engine 是用来处理 Nebula 查询语言语句（nGQL）。本篇文章将带你了解 Nebula Query Engine 的架构。
上图为查询引擎的架构图，如果你对 SQL 的执行引擎比较熟悉，那么对上图一定不会陌生。Nebula 的 Query Engine 架构图和现代 SQL 的执行引擎类似，只是在查询语言解析器和具体的执行计划有所区别。
Session Manager Nebula 权限管理采用基于角色的权限控制（Role Based Access Control）。客户端第一次连接到 Query Engine 时需作认证，当认证成功之后 Query Engine 会创建一个新 session，并将该 session ID 返回给客户端。所有的 session 统一由 Session Manger 管理。session 会记录当前 graph space 信息及对该 space 的权限。此外，session 还会记录一些会话相关的配置信息，并临时保存同一 session 内的跨多个请求的一些信息。
客户端连接结束之后 session 会关闭，或者如果长时间没通信会切为空闲状态。这个空闲时长是可以配置的。客户端的每个请求都必须带上此 session ID，否则 Query Engine 会拒绝此请求。
Storage Engine 不管理 session，Query Engine 在访问存储引擎时，会带上 session 信息。</description></item><item><title>Nebula 架构剖析系列（一）图数据库的存储设计</title><link>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-engine-overview/</link><pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate><guid>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-engine-overview/</guid><description>摘要 在讨论某个数据库时，存储 ( Storage ) 和计算 ( Query Engine ) 通常是讨论的热点，也是爱好者们了解某个数据库不可或缺的部分。每个数据库都有其独有的存储、计算方式，今天就和图图来学习下图数据库 Nebula Graph 的存储部分。
Nebula 的 Storage 包含两个部分， 一是 meta 相关的存储， 我们称之为 Meta Service ，另一个是 data 相关的存储， 我们称之为 Storage Service。 这两个服务是两个独立的进程，数据也完全隔离，当然部署也是分别部署， 不过两者整体架构相差不大，本文最后会提到这点。 如果没有特殊说明，本文中 Storage Service 代指 data 的存储服务。接下来，大家就随我一起看一下 Storage Service 的整个架构。 Let&amp;rsquo;s go~
Architecture 图一 storage service 架构图
如图1 所示，Storage Service 共有三层，最底层是 Store Engine，它是一个单机版 local store engine，提供了对本地数据的 get / put / scan / delete 操作，相关的接口放在 KVStore / KVEngine.h 文件里面，用户完全可以根据自己的需求定制开发相关 local store plugin，目前 Nebula 提供了基于 RocksDB 实现的 Store Engine。</description></item><item><title>Nebula 架构剖析系列（零）图数据库的整体架构设计</title><link>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-architecture-overview/</link><pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate><guid>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-architecture-overview/</guid><description>Nebula Graph 是一个高性能的分布式开源图数据库，本文为大家介绍 Nebula Graph 的整体架构。
一个完整的 Nebula 部署集群包含三个服务，即 Query Service，Storage Service 和 Meta Service。每个服务都有其各自的可执行二进制文件，这些二进制文件既可以部署在同一组节点上，也可以部署在不同的节点上。
Meta Service 上图为 Nebula Graph 的架构图，其右侧为 Meta Service 集群，它采用 leader / follower 架构。Leader 由集群中所有的 Meta Service 节点选出，然后对外提供服务。Followers 处于待命状态并从 leader 复制更新的数据。一旦 leader 节点 down 掉，会再选举其中一个 follower 成为新的 leader。
Meta Service 不仅负责存储和提供图数据的 meta 信息，如 schema、partition 信息等，还同时负责指挥数据迁移及 leader 的变更等运维操作。
存储计算分离 在架构图中 Meta Service 的左侧，为 Nebula Graph 的主要服务，Nebula 采用存储与计算分离的架构，虚线以上为计算，以下为存储。
存储计算分离有诸多优势，最直接的优势就是，计算层和存储层可以根据各自的情况弹性扩容、缩容。
存储计算分离还带来的另一个优势：使水平扩展成为可能。
此外，存储计算分离使得 Storage Service 可以为多种类型的个计算层或者计算引擎提供服务。当前 Query Service 是一个高优先级的计算层，而各种迭代计算框架会是另外一个计算层。
无状态计算层 现在我们来看下计算层，每个计算节点都运行着一个无状态的查询计算引擎，而节点彼此间无任何通信关系。计算节点仅从 Meta Service 读取 meta 信息，以及和 Storage Service 进行交互。这样设计使得计算层集群更容易使用 K8s 管理或部署在云上。</description></item><item><title>图数据库 Nebula Graph 的数据模型和系统架构设计</title><link>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-data-model-and-system-design/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-data-model-and-system-design/</guid><description>Nebula Graph：一个开源的分布式图数据库。作为唯一能够存储万亿个带属性的节点和边的在线图数据库，Nebula Graph 不仅能够在高并发场景下满足毫秒级的低时延查询要求，还能够实现服务高可用且保障数据安全性。
本篇主要介绍 Nebula Graph 的数据模型和系统架构设计。
有向属性图 DirectedPropertyGraph Nebula Graph 采用易理解的有向属性图来建模，也就是说，在逻辑上，图由两种图元素构成：顶点和边。
顶点 Vertex 在 Nebula Graph 中顶点由标签 tag 和对应 tag 的属性组构成， tag 代表顶点的类型，属性组代表 tag 拥有的一种或多种属性。一个顶点必须至少有一种类型，即标签，也可以有多种类型。每种标签有一组相对应的属性，我们称之为 schema 。
如上图所示，有两种 tag 顶点：player 和 team。player 的 schema 有三种属性 ID （vid），Name （sting）和 Age （int）；team 的 schema 有两种属性 ID （vid）和 Name （string）。
和 Mysql 一样，Nebula Graph 是一种强 schema 的数据库，属性的名称和数据类型都是在数据写入前确定的。
边 Edge 在 Nebula Graph 中边由类型和边属性构成，而 Nebula Graph 中边均是有向边，有向边表明一个顶点（ 起点 src ）指向另一个顶点（ 终点 dst ）的关联关系。此外，在 Nebula Graph 中我们将边类型称为 edgetype ，每一条边只有一种edgetype ，每种 edgetype 相应定义了这种边上属性的 schema 。</description></item></channel></rss>