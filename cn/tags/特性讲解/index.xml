<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>特性讲解 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</title><link>aseURL=vesoft-inc.github.io/nebula-website/cn/tags/%E7%89%B9%E6%80%A7%E8%AE%B2%E8%A7%A3/</link><description>Recent content in 特性讲解 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 24 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="aseURL=vesoft-inc.github.io/nebula-website/cn/tags/%E7%89%B9%E6%80%A7%E8%AE%B2%E8%A7%A3/index.xml" rel="self" type="application/rss+xml"/><item><title>图数据库 Nebula Graph TTL 特性</title><link>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/clean-stale-data-with-ttl-in-nebula-graph/</link><pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate><guid>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/clean-stale-data-with-ttl-in-nebula-graph/</guid><description>导读 身处在现在这个大数据时代，我们处理的数据量需以 TB、PB, 甚至 EB 来计算，怎么处理庞大的数据集是从事数据库领域人员的共同问题。解决这个问题的核心在于，数据库中存储的数据是否都是有效的、有用的数据，因此如何提高数据中有效数据的利用率、将无效的过期数据清洗掉，便成了数据库领域的一个热点话题。在本文中我们将着重讲述如何在数据库中处理过期数据这一问题。
在数据库中清洗过期数据的方式多种多样，比如存储过程、事件等等。在这里笔者举个例子来简要说明 DBA 经常使用的存储过程 + 事件来清理过期数据的过程。
存储过程 + 事件清洗数据 存储过程（procedure） 存储过程是由一条或多条 SQL 语句组成的集合，当对数据库进行一系列的读写操作时，存储过程可将这些复杂的操作封装成一个代码块以便重复使用，大大减少了数据库开发人员的工作量。通常存储过程编译一次，可以执行多次，因此也大大的提高了效率。
存储过程有以下优点：
简化操作，将重复性很高的一些操作，封装到一个存储过程中，简化了对这些 SQL 的调用 批量处理，SQL + 循环，减少流量，也就是“跑批” 统一接口，确保数据的安全 一次编译多次执行，提高了效率。 以 MySQL 为例，假如要删除数据的表结构如下：
mysql&amp;gt; SHOW CREATE TABLE person; +--------+---------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +--------+---------------------------------------------------------------------------------------------------------------------------------+ | person | CREATE TABLE `person` ( `age` int(11) DEFAULT NULL, `inserttime` datetime DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +--------+---------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.</description></item><item><title>分布式图数据库 Nebula Graph 的 Index 实践</title><link>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/how-indexing-works-in-nebula-graph/</link><pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate><guid>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/how-indexing-works-in-nebula-graph/</guid><description>导读 索引是数据库系统中不可或缺的一个功能，数据库索引好比是书的目录，能加快数据库的查询速度，其实质是数据库管理系统中一个排序的数据结构。不同的数据库系统有不同的排序结构，目前常见的索引实现类型如 B-Tree index、B+-Tree index、B*-Tree index、Hash index、Bitmap index、Inverted index 等等，各种索引类型都有各自的排序算法。
虽然索引可以带来更高的查询性能，但是也存在一些缺点，例如：
创建索引和维护索引要耗费额外的时间,往往是随着数据量的增加而维护成本增大 索引需要占用物理空间 在对数据进行增删改的操作时需要耗费更多的时间,因为索引也要进行同步的维护 Nebula Graph 作为一个高性能的分布式图数据库，对于属性值的高性能查询，同样也实现了索引功能。本文将对 Nebula Graph 的索引功能做一个详细介绍。
图数据库 Nebula Graph 术语 开始之前，这里罗列一些可能会使用到的图数据库和 Nebula Graph 专有术语：
Tag：点的属性结构，一个 Vertex 可以附加多种 tag，以 TagID 标识。（如果类比 SQL，可以理解为一张点表） Edge：类似于 Tag，EdgeType 是边上的属性结构，以 EdgeType 标识。（如果类比 SQL，可以理解为一张边表） Property：tag / edge 上的属性值，其数据类型由 tag / edge 的结构确定。 Partition：Nebula Graph 的最小逻辑存储单元，一个 StorageEngine 可包含多个 Partition。Partition 分为 leader 和 follower 的角色，Raftex 保证了 leader 和 follower 之间的数据一致性。 Graph space：每个 Graph Space 是一个独立的业务 Graph 单元，每个 Graph Space 有其独立的 tag 和 edge 集合。一个 Nebula Graph 集群中可包含多个 Graph Space。 Index：本文中出现的 Index 指 nebula graph 中点和边上的属性索引。其数据类型依赖于 tag / edge。 TagIndex：基于 tag 创建的索引，一个 tag 可以创建多个索引。目前（2020.</description></item><item><title>图数据库设计实践 | 存储服务的负载均衡和数据迁移</title><link>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</guid><description>在文章《Nebula 架构剖析系列（一）图数据库的存储设计》中，我们提过分布式图存储的管理由 Meta Service 来统一调度，它记录了所有 partition 的分布情况，以及当前机器的状态。当 DBA 增减机器时，只需要通过 console 输入相应的指令，Meta Service 便能够生成整个 Balance 计划并执行。而之所以没有采用完全自动 Balance 的方式，主要是为了减少数据搬迁对于线上服务的影响，Balance 的时机由用户自己控制。
在本文中我们将着重讲解在存储层如何实现数据和服务的负载平衡。
简单回顾一下，Nebula Graph 的服务可分为 graph，storage，meta。本文主要描述对于存储层（storage）的数据和服务的 balance。这些都是通过 Balance 命令来实现的：Balance 命令有两种，一种需要迁移数据，命令为 BALANCE DATA ；另一种不需要迁移数据，只改变 partition 的 raft-leader 分布（负载均衡），命令为 BALANCE LEADER 。
本文目录 Balance 机制浅析 集群数据迁移 Step 1：准备工作 Step 1.1 查看现有集群状态 Step 1.2 创建图空间 Step 2 加入新实例 Step 3 迁移数据 Step 4 假如要中途停止 balance data Step 5 查看数据迁移结果 Step 6 Balance leader 批量缩容 示例数据迁移 Balance 机制浅析 在图数据库 Nebula Graph 中， Balance 主要用来 balance leader 和 partition，只涉及 leader 和 partition 在机器之间转移，不会增加或者减少 leader 和 partition 的数量。</description></item><item><title>分布式图数据库 Nebula Graph 中的集群快照实践</title><link>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/introduction-to-snapshot-in-nebula-graph/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>aseURL=vesoft-inc.github.io/nebula-website/cn/posts/introduction-to-snapshot-in-nebula-graph/</guid><description>1 概述 1.1 需求背景 图数据库 Nebula Graph 在生产环境中将拥有庞大的数据量和高频率的业务处理，在实际的运行中将不可避免的发生人为的、硬件或业务处理错误的问题，某些严重错误将导致集群无法正常运行或集群中的数据失效。当集群处于无法启动或数据失效的状态时，重新搭建集群并重新倒入数据都将是一个繁琐并耗时的工程。针对此问题，Nebula Graph 提供了集群 snapshot 的创建功能。
Snapshot 功能需要预先提供集群在某个时间点 snapshot 的创建功能，以备发生灾难性问题时用历史 snapshot 便捷地将集群恢复到一个可用状态。
1.2 术语 本文主要会用到以下术语：
StorageEngine：Nebula Graph 的最小物理存储单元，目前支持 RocksDB 和 HBase，在本文中只针对 RocksDB。 Partition：Nebula Graph 的最小逻辑存储单元，一个 StorageEngine 可包含多个 Partition。Partition 分为 leader 和 follower 的角色，Raftex 保证了 leader 和 follower 之间的数据一致性。 GraphSpace：每个 GraphSpace 是一个独立的业务 Graph 单元，每个 GraphSpace 有其独立的 tag 和 edge 集合。一个 Nebula Graph 集群中可包含多个 GraphSpace。 checkpoint：针对 StorageEngine 的一个时间点上的快照，checkpoint 可以作为全量备份的一个 backup 使用。checkpoint files是 sst files 的一个硬连接。 snapshot：本文中的 snapshot 是指 Nebula Graph 集群的某个时间点的快照，即集群中所有 StorageEngine 的 checkpoint 的集合。通过 snapshot 可以将集群恢复到某个 snapshot 创建时的状态。 wal：Write-Ahead Logging ，用 raftex 保证 leader 和 follower 的一致性。 2 系统构架 2.</description></item></channel></rss>