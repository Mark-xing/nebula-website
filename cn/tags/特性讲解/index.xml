<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>特性讲解 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</title><link>/nebula-website/cn/tags/%E7%89%B9%E6%80%A7%E8%AE%B2%E8%A7%A3/</link><description>Recent content in 特性讲解 on Nebula Graph - An Open Source, Distributed and High Performant Graph Database</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 29 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="/nebula-website/cn/tags/%E7%89%B9%E6%80%A7%E8%AE%B2%E8%A7%A3/index.xml" rel="self" type="application/rss+xml"/><item><title>D3.js 力导向图的显示优化</title><link>/nebula-website/cn/posts/d3-force-layout-optimization/</link><pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/d3-force-layout-optimization/</guid><description>D3.js 作为一个前端，说到可视化除了听过 D3.js 的大名，常见的可视化库还有 ECharts、Chart.js，这两个库功能也很强大，但是有一个共同特点是封装层次高，留给开发者可设计和控制的部分太少。和 EChart、Chart.js 等相比，D3.js** **的相对来说自由度会高很多，得益于 D3.js** **中的 **SVG 画图对事件处理器的支持**，D3.js 可将任意数据绑定到文档对象模型（DOM）上，也可以直接操作对象模型（DOM）完成 W3C DOM API 相关操作，对于想要展示自己设计图形的开发者，D3.js 绝对是一个不错的选择。
d3-force 力导向图 以实现一个关系网来说，d3-force 力导向图是不二的选择。d3-force 是 D3.js 实现以模拟粒子物理运动的 velocity Verlet 数值积分器的模块，可用来控制粒子和边秩序。在力导向图中，d3-force 中的每个节点都可以看成是一个放电粒子，粒子间存在某种斥力（库仑斥力）。同时，这些粒子间被它们之间的“边”所牵连，从而产生牵引力。
而 d3-force 中的粒子在斥力和牵引力的作用下，从随机无序的初态不断发生位移，逐渐趋于平衡有序。整个图只有点 / 边，图形实现样例较少且自定义样式居多。
下图就是最简单的关系网图，想要实现自己想要的关系网图，还是动手自己实现一个 D3.js 力导向图最佳。
构建 D3.js 力导向图 在这里实践过程中，我们用 D3.js 力导向图来对图数据库的数据关系进行分析，其节点和关系线直观地体现出图数据库的数据关系，并且还可以关联相对应的图数据库语句完成拓展查询。进阶来说，可通过对文档对象模型（DOM）的直接操作同步到数据库进而更新数据，当然操作这个比较复杂，😂 不在本文中详细讲述。
下面，我们来实现一个简单的力导向图，初窥 D3.js 对数据分析的作用和显示优化的一些思路。首先我们创建一个力导向图：
this.force = d3 .forceSimulation() // 为节点分配坐标 .nodes(data.vertexes) // 连接线 .force('link', linkForce) // 整个实例中心 .force('center', d3.forceCenter(width / 2, height / 2)) // 引力 .force('charge', d3.</description></item><item><title>图数据库 Nebula Graph 的代码变更测试覆盖率实践</title><link>/nebula-website/cn/posts/integrate-codecov-test-coverage-with-nebula-graph/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/integrate-codecov-test-coverage-with-nebula-graph/</guid><description>对于一个持续开发的大型工程而言，足够的测试是保证软件行为符合预期的有效手段，而不是仅仅依靠 code review 或者开发者自己的技术素质。测试的编写理想情况下应该完全定义软件的行为，但是通常情况都是很难达到这样理想的程度。而测试覆盖率就是检验测试覆盖软件行为的情况，通过检查测试覆盖情况可以帮助开发人员发现没有被覆盖到的代码。
测试覆盖信息搜集 Nebula Graph 主要是由 C++ 语言开发的，支持大部分 Linux 环境以及 gcc/clang 编译器，所以通过工具链提供的支持，我们可以非常方便地统计Nebula Graph的测试覆盖率。
gcc/clang 都支持 gcov 式的测试覆盖率功能，使用起来也是非常简单的，主要有如下几个步骤：
添加编译选项 --coverage -O0 -g  添加链接选项 --coverage  运行测试 使用 lcov，整合报告，例如 lcov --capture --directory . --output-file coverage.info  去掉外部代码统计，例如 lcov --remove coverage.info '*/opt/vesoft/*' -o clean.info  到这里测试覆盖信息已经搜集完毕，接下可以通过 genhtml 这样的工具生成 html，然后通过浏览器查看测试覆盖率，如下图所示：
但是这样是非常不方便的，因为在持续的开发过程，如果每次都要手动进行这样一套操作，那必然带来极大的人力浪费，所以现在的常用做法是将测试覆盖率写入 CI 并且和第三方平台（比如 Codecov，Coveralls）集成，这样开发人员完全不必关心测试覆盖信息的收集整理和展示问题，只需要发布代码后直接到第三方平台上查看覆盖情况即可，而且现在的第三方平台也支持直接在 PR 上评论覆盖情况使得查看覆盖率的变更情况更加方便。
集成 CI Github Action 现在主流的 CI 平台非常多，比如 Travis，azure-pipelines 以及 GitHub Action 等。Nebula Graph 选用的是 GitHub Action，对于 Action 我们在之前的《使用 Github Action 进行前端自动化发布》这篇文章里已经做过介绍。</description></item><item><title>基于 Jepsen 来发现几个 Raft 实现中的一致性问题(2)</title><link>/nebula-website/cn/posts/detect-data-consistency-issues-in-raft-implementing-with-jepsen/</link><pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/detect-data-consistency-issues-in-raft-implementing-with-jepsen/</guid><description>Nebula Graph 是一个高性能、高可用、强一致的分布式图数据库。由于 Nebula Graph 采用的是存储计算分离架构，在存储层实际只是暴露了简单的 kv 接口，采用 RocksDB 作为状态机，通过 Raft 一致性协议来保证多副本数据一致的问题。Raft 协议虽然比 Paxos 更加容易理解，但在工程实现上还是有很多需要注意和优化的地方。
另外，如何测试基于 Raft 的分布式系统也是困扰业界的问题，目前 Nebula 主要采用了 Jepsen 作为一致性验证工具。之前我的小伙伴已经在《Jepsen 测试框架在图数据库 Nebula Graph 中的实践》中做了详细的介绍，对 Jepsen 不太了解的同学可以先移步这篇文章。
在这篇文章中将着重介绍如何通过 Jepsen 来对 Nebula Graph 的分布式 kv 进行一致性验证。
强一致的定义 首先，我们需要什么了解叫强一致，它实际就是 Linearizability，也被称为线性一致性。引用《Designing Data-Intensive Applications》里一书里的定义：
In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written.</description></item><item><title>图数据库 Nebula Graph TTL 特性</title><link>/nebula-website/cn/posts/clean-stale-data-with-ttl-in-nebula-graph/</link><pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/clean-stale-data-with-ttl-in-nebula-graph/</guid><description>导读 身处在现在这个大数据时代，我们处理的数据量需以 TB、PB, 甚至 EB 来计算，怎么处理庞大的数据集是从事数据库领域人员的共同问题。解决这个问题的核心在于，数据库中存储的数据是否都是有效的、有用的数据，因此如何提高数据中有效数据的利用率、将无效的过期数据清洗掉，便成了数据库领域的一个热点话题。在本文中我们将着重讲述如何在数据库中处理过期数据这一问题。
在数据库中清洗过期数据的方式多种多样，比如存储过程、事件等等。在这里笔者举个例子来简要说明 DBA 经常使用的存储过程 + 事件来清理过期数据的过程。
存储过程 + 事件清洗数据 存储过程（procedure） 存储过程是由一条或多条 SQL 语句组成的集合，当对数据库进行一系列的读写操作时，存储过程可将这些复杂的操作封装成一个代码块以便重复使用，大大减少了数据库开发人员的工作量。通常存储过程编译一次，可以执行多次，因此也大大的提高了效率。
存储过程有以下优点：
简化操作，将重复性很高的一些操作，封装到一个存储过程中，简化了对这些 SQL 的调用 批量处理，SQL + 循环，减少流量，也就是“跑批” 统一接口，确保数据的安全 一次编译多次执行，提高了效率。 以 MySQL 为例，假如要删除数据的表结构如下：
mysql&amp;gt; SHOW CREATE TABLE person; +--------+---------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +--------+---------------------------------------------------------------------------------------------------------------------------------+ | person | CREATE TABLE `person` ( `age` int(11) DEFAULT NULL, `inserttime` datetime DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +--------+---------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.</description></item><item><title>分布式图数据库 Nebula Graph 的 Index 实践</title><link>/nebula-website/cn/posts/how-indexing-works-in-nebula-graph/</link><pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/how-indexing-works-in-nebula-graph/</guid><description>导读 索引是数据库系统中不可或缺的一个功能，数据库索引好比是书的目录，能加快数据库的查询速度，其实质是数据库管理系统中一个排序的数据结构。不同的数据库系统有不同的排序结构，目前常见的索引实现类型如 B-Tree index、B+-Tree index、B*-Tree index、Hash index、Bitmap index、Inverted index 等等，各种索引类型都有各自的排序算法。
虽然索引可以带来更高的查询性能，但是也存在一些缺点，例如：
创建索引和维护索引要耗费额外的时间,往往是随着数据量的增加而维护成本增大 索引需要占用物理空间 在对数据进行增删改的操作时需要耗费更多的时间,因为索引也要进行同步的维护 Nebula Graph 作为一个高性能的分布式图数据库，对于属性值的高性能查询，同样也实现了索引功能。本文将对 Nebula Graph 的索引功能做一个详细介绍。
图数据库 Nebula Graph 术语 开始之前，这里罗列一些可能会使用到的图数据库和 Nebula Graph 专有术语：
Tag：点的属性结构，一个 Vertex 可以附加多种 tag，以 TagID 标识。（如果类比 SQL，可以理解为一张点表） Edge：类似于 Tag，EdgeType 是边上的属性结构，以 EdgeType 标识。（如果类比 SQL，可以理解为一张边表） Property：tag / edge 上的属性值，其数据类型由 tag / edge 的结构确定。 Partition：Nebula Graph 的最小逻辑存储单元，一个 StorageEngine 可包含多个 Partition。Partition 分为 leader 和 follower 的角色，Raftex 保证了 leader 和 follower 之间的数据一致性。 Graph space：每个 Graph Space 是一个独立的业务 Graph 单元，每个 Graph Space 有其独立的 tag 和 edge 集合。一个 Nebula Graph 集群中可包含多个 Graph Space。 Index：本文中出现的 Index 指 nebula graph 中点和边上的属性索引。其数据类型依赖于 tag / edge。 TagIndex：基于 tag 创建的索引，一个 tag 可以创建多个索引。目前（2020.</description></item><item><title>图数据库设计实践 | 存储服务的负载均衡和数据迁移</title><link>/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/nebula-graph-storage-banlancing-data-migration/</guid><description>在文章《Nebula 架构剖析系列（一）图数据库的存储设计》中，我们提过分布式图存储的管理由 Meta Service 来统一调度，它记录了所有 partition 的分布情况，以及当前机器的状态。当 DBA 增减机器时，只需要通过 console 输入相应的指令，Meta Service 便能够生成整个 Balance 计划并执行。而之所以没有采用完全自动 Balance 的方式，主要是为了减少数据搬迁对于线上服务的影响，Balance 的时机由用户自己控制。
在本文中我们将着重讲解在存储层如何实现数据和服务的负载平衡。
简单回顾一下，Nebula Graph 的服务可分为 graph，storage，meta。本文主要描述对于存储层（storage）的数据和服务的 balance。这些都是通过 Balance 命令来实现的：Balance 命令有两种，一种需要迁移数据，命令为 BALANCE DATA ；另一种不需要迁移数据，只改变 partition 的 raft-leader 分布（负载均衡），命令为 BALANCE LEADER 。
本文目录 Balance 机制浅析 集群数据迁移 Step 1：准备工作 Step 1.1 查看现有集群状态 Step 1.2 创建图空间 Step 2 加入新实例 Step 3 迁移数据 Step 4 假如要中途停止 balance data Step 5 查看数据迁移结果 Step 6 Balance leader 批量缩容 示例数据迁移 Balance 机制浅析 在图数据库 Nebula Graph 中， Balance 主要用来 balance leader 和 partition，只涉及 leader 和 partition 在机器之间转移，不会增加或者减少 leader 和 partition 的数量。</description></item><item><title>分布式图数据库 Nebula Graph 中的集群快照实践</title><link>/nebula-website/cn/posts/introduction-to-snapshot-in-nebula-graph/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>/nebula-website/cn/posts/introduction-to-snapshot-in-nebula-graph/</guid><description>1 概述 1.1 需求背景 图数据库 Nebula Graph 在生产环境中将拥有庞大的数据量和高频率的业务处理，在实际的运行中将不可避免的发生人为的、硬件或业务处理错误的问题，某些严重错误将导致集群无法正常运行或集群中的数据失效。当集群处于无法启动或数据失效的状态时，重新搭建集群并重新倒入数据都将是一个繁琐并耗时的工程。针对此问题，Nebula Graph 提供了集群 snapshot 的创建功能。
Snapshot 功能需要预先提供集群在某个时间点 snapshot 的创建功能，以备发生灾难性问题时用历史 snapshot 便捷地将集群恢复到一个可用状态。
1.2 术语 本文主要会用到以下术语：
StorageEngine：Nebula Graph 的最小物理存储单元，目前支持 RocksDB 和 HBase，在本文中只针对 RocksDB。 Partition：Nebula Graph 的最小逻辑存储单元，一个 StorageEngine 可包含多个 Partition。Partition 分为 leader 和 follower 的角色，Raftex 保证了 leader 和 follower 之间的数据一致性。 GraphSpace：每个 GraphSpace 是一个独立的业务 Graph 单元，每个 GraphSpace 有其独立的 tag 和 edge 集合。一个 Nebula Graph 集群中可包含多个 GraphSpace。 checkpoint：针对 StorageEngine 的一个时间点上的快照，checkpoint 可以作为全量备份的一个 backup 使用。checkpoint files是 sst files 的一个硬连接。 snapshot：本文中的 snapshot 是指 Nebula Graph 集群的某个时间点的快照，即集群中所有 StorageEngine 的 checkpoint 的集合。通过 snapshot 可以将集群恢复到某个 snapshot 创建时的状态。 wal：Write-Ahead Logging ，用 raftex 保证 leader 和 follower 的一致性。 2 系统构架 2.</description></item></channel></rss>