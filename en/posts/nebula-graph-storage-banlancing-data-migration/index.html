<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Nebula Graph"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@NebulaGraph"><meta name=twitter:creator content="@NebulaGraph"><meta name=twitter:domain content="nebula-graph.io"><meta name=twitter:title content="Storage Balance and Data Migration"><meta property="twitter:title" content="Storage Balance and Data Migration"><meta name=twitter:description content="This post explains in detail how Nebula Graph handles data balancing and data migration at the storage layer."><meta name=og:description content="This post explains in detail how Nebula Graph handles data balancing and data migration at the storage layer."><meta name=twitter:image content="/aseURL=vesoft-inc.github.io/nebula-website/en/posts/nebula-graph-storage-banlancing-data-migration/databalance_hudd46e824bed743626710666f9b8a7ab2_189828_600x0_resize_q75_box.jpg"><meta name=og:image content="/aseURL=vesoft-inc.github.io/nebula-website/en/posts/nebula-graph-storage-banlancing-data-migration/databalance_hudd46e824bed743626710666f9b8a7ab2_189828_600x0_resize_q75_box.jpg"><meta name=description content="This post explains in detail how Nebula Graph handles data balancing and data migration at the storage layer."><meta name=generator content="Hugo 0.65.3"><title>Storage Balance and Data Migration</title><link rel="shortcut icon" href="/aseURL=vesoft-inc.github.io/nebula-website/favicon.ico"><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/css/bootstrap.min.css" type=text/css><link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel=stylesheet type=text/css><link href="//fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic" rel=stylesheet type=text/css><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/font-awesome/css/font-awesome.min.css" type=text/css><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/css/creative.css" type=text/css><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/css/modals.css" type=text/css><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/css/animate.min.css" type=text/css><link rel=stylesheet href="/aseURL=vesoft-inc.github.io/nebula-website/css/custom/index.css" type=text/css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js></script><script src=https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js></script><![endif]--><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/jquery.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-60523578-5"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-60523578-5');</script></head><body id=page-top><nav id=mainNav class="navbar navbar-default navbar-fixed-top"><div class=container-fluid><div class=navbar-header><div class=github-star><a class=github-button href=https://github.com/vesoft-inc/nebula data-icon=octicon-star data-show-count=true aria-label="Star vesoft-inc/nebula on GitHub">Star</a></div><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=#bs-example-navbar-collapse-1>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class="navbar-brand page-scroll" href=/></a></div><div class="collapse navbar-collapse" id=bs-example-navbar-collapse-1><ul class="nav navbar-nav" id=nav><li class=github-star id=github-star><a class=github-button href=https://github.com/vesoft-inc/nebula data-size=large data-show-count=true aria-label="Star vesoft-inc/nebula on GitHub">Star</a></li><li><a href=https://docs.nebula-graph.io/ target=_blank>Docs</a></li><li><a href=/en/posts target>Blog</a></li><li><a href=https://discuss.nebula-graph.io/ target=_blank>Forum</a></li><li><a href=#contact_us target onclick=contactUsClick()>Contact Us</a></li><li class="dropdown navbar-right"><a class=dropdown-toggle data-toggle=dropdown href=# role=button aria-haspopup=true aria-expanded=false><img src=/images/language120x120.png>
<span class=caret></span></a><ul class=dropdown-menu><li class=nav-item><a class=dropdown-item href=/en target>English</a></li><li class=nav-item><a class=dropdown-item href=/cn target>中文</a></li></ul></li><li id=navbar-right><a href=https://github.com/vesoft-inc/nebula-web-docker target=_blank>Nebula Studio</a></li></ul></div></div></nav><script>function contactUsClick(){$('html,body').animate({scrollTop:document.documentElement.scrollHeight},800)
return false;}</script><main id=top class=blog-detail><div class=wrapper><div class=inner><section class=blog-content><h1>Storage Balance and Data Migration</h1><div class=blog-metas><div class=meta><img src=/images/writer.png width=16px height=16px>
<span>Heng Chen</span></div><div class=meta><img src=/images/calendar.png width=16px height=16px>
<span>2020-02-04</span></div><div class="tags meta"><img src=/images/tag.png width=16px height=16px>
<a href="/aseURL=vesoft-inc.github.io/nebula-website/en//tags/Features">Features</a></div></div><p><img src=https://user-images.githubusercontent.com/56643819/73818163-b7098400-4827-11ea-90b9-239d4e1c1285.png alt=image></p><p>In our post <a href=https://github.com/vesoft-inc/nebula/blob/master/docs/manual-EN/1.overview/3.design-and-architecture/2.storage-design.md><em>Storage Design</em></a> we mentioned the distributed kv store is managed by the meta service. Both the partition distribution and machine status can be found in the meta service. Users can <del>input</del> use commands in the console to add or remove machines to execute a balance plan for the storage service.</p><p><strong>Nebula Graph</strong>&lsquo;s service is composed of three parts: graph, storage and meta. In this post, we will introduce how to implement data (partition) and work-load balance in the storage service.</p><p>The storage service can be scaled in or out horizontally by the <code>BALANCE</code> commands below: </p><ul><li><code>BALANCE DATA</code>  is used to migrate data from old machines to new machines</li><li><code>BALANCE LEADER</code> only changes the distribution of leader partition to balance the work load without moving data</li></ul><h2 id=table-of-contents>Table of Contents</h2><ul><li>Intro to the balance mechanism</li><li>Cluster data migration<ul><li>Step 1: Prerequisites<ul><li>Step 1.1 Show the current cluster status</li><li>Step 1.2 Create graph spaces</li></ul></li><li>Step 2 Add new hosts</li><li>Step 3 Data migration</li><li>Step 4 If stop data balance halfway, &mldr;</li><li>Step 5 Data migration is done</li><li>Step 6 Next, balance leader</li></ul></li><li>Batch scale in</li><li>Conclution</li></ul><h2 id=intro-to-the-balance-mechanism>Intro to the balance mechanism</h2><p>In <strong>Nebula Graph</strong> balance means to balance both the raft leader and partition data. But the balance** does not change the numbers of leaders or partitions**.</p><p>When you add a new machine with Nebula service, the (new) storage will automatically register to the Meta service. Meta calculates an equally partition distribution, and then uses <strong>remove partition</strong> and <strong>add partition</strong> to make those partitions distributed evenly. The corresponding command is <code>BALANCE DATA</code>. Usually, the data migration is a time-consuming process.</p><p>However, <code>BALANCE DATA</code> only changes the replica distribution among the machines. But the leaders (corresponding work load) will not be changed. Next, you need to use the <code>BALANCE LEADER</code> command to achieve load balance. This process is also implemented through the meta service.</p><h2 id=cluster-data-migration>Cluster data migration</h2><p>The following example will show how to expand the cluster from three instances to eight instances.</p><h3 id=step-1-prerequisites>Step 1 Prerequisites</h3><p>Suppose you&rsquo;ve already start a cluster with three replicas.</p><h4 id=step-11-show-the-current-cluster-status>Step 1.1 Show the current cluster status</h4><p>Show the current status with command <code>SHOW HOSTS:</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nebula&gt; SHOW HOSTS
<span style=color:#f92672>================================================================================================</span>
| Ip            | Port  | Status | Leader count | Leader distribution | Partition distribution |
<span style=color:#f92672>================================================================================================</span>
| 192.168.8.210 | <span style=color:#ae81ff>34600</span> | online | <span style=color:#ae81ff>0</span>            | No valid partition  | No valid partition     |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34700</span> | online | <span style=color:#ae81ff>0</span>            | No valid partition  | No valid partition     |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34500</span> | online | <span style=color:#ae81ff>0</span>            | No valid partition  | No valid partition     |
------------------------------------------------------------------------------------------------
Got <span style=color:#ae81ff>3</span> rows <span style=color:#f92672>(</span>Time spent: 5886/6835 us<span style=color:#f92672>)</span>
</code></pre></div><p><strong><em>Explanations on the returned results:</em></strong></p><ul><li><strong><em>IP</em></strong> and <strong><em>Port</em></strong> are the storage instance. The cluster has three storaged instances (192.168.8.210:34600, 192.168.8.210:34700, 192.168.8.210:34500) without any data.</li><li><strong><em>Status</em></strong> shows the state of each instance. There are two kinds of states, i.e. online/offline. When a host crashed, metad will turn it to offline after its heart beat timed out. The default heart beat threshold is 10 minutes (You can find parameter <code>expired_threshold_sec</code> meta&rsquo;s config file).</li><li><strong><em>Leader count</em></strong> shows the number of raft leader of the instance served.</li><li><strong><em>Leader distribution</em></strong> shows how the leader distributed in each graph space. For now there are no spaces  created.  (You can regard space as an independent name space &ndash; similar to the Database in MySQL.)</li><li><strong><em>Partition distribution</em></strong> shows the partition number of different spaces.</li></ul><p><img src=https://user-images.githubusercontent.com/56643819/73815505-26c84080-4821-11ea-8cfa-46fa202462f5.png alt=image>We can see there is no data in the <em>Leader distribution</em> and <em>Partition distribution</em> for the time.</p><h4 id=step-12-create-a-graph-space>Step 1.2 Create a graph space</h4><p>Create a graph space named <code>**test</code>** with 100 partition and 3 replicas.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nebula&gt; CREATE SPACE test<span style=color:#f92672>(</span>PARTITION_NUM<span style=color:#f92672>=</span>100, REPLICA_FACTOR<span style=color:#f92672>=</span>3<span style=color:#f92672>)</span>
</code></pre></div><p>After a few seconds, run the command <code>SHOW HOSTS</code>again:</p><pre><code>nebula&gt; SHOW HOSTS
================================================================================================
| Ip            | Port  | Status | Leader count | Leader distribution | Partition distribution |
================================================================================================
| 192.168.8.210 | 34600 | online | 0            | test: 0             | test: 100              |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34700 | online | 52           | test: 52            | test: 100              |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34500 | online | 48           | test: 48            | test: 100              |
------------------------------------------------------------------------------------------------
</code></pre><p><img src=https://user-images.githubusercontent.com/56643819/73818168-bcff6500-4827-11ea-9924-b12919acd489.png alt=image>After we created the space <code>test</code> with 100  _partitio_ns and 3 replicas, the host192.168.8.210:34600 serves NO leader, while 192.168.8.210:34700 serves 52 leaders and 192.168.8.210 serves 48 leaders。The leaders are not equilly distributed.</p><h3 id=step-2-add-five-new-instances>Step 2 Add five new instances</h3><p>Now, let&rsquo;s add five new instances into the cluster.</p><p>Again, show the new status using statement <code>SHOW HOSTS</code>. You can see there are already eight instances in serving. But no partition is running on the new instances.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>nebula<span style=color:#f92672>&gt;</span> SHOW HOSTS
<span style=color:#f92672>================================================================================================</span>
<span style=color:#f92672>|</span> Ip            <span style=color:#f92672>|</span> Port  <span style=color:#f92672>|</span> Status <span style=color:#f92672>|</span> Leader count <span style=color:#f92672>|</span> Leader distribution <span style=color:#f92672>|</span> Partition distribution <span style=color:#f92672>|</span>
<span style=color:#f92672>================================================================================================</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34600</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>0</span>             <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>100</span>              <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34900</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> No valid partition  <span style=color:#f92672>|</span> No valid partition     <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>35940</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> No valid partition  <span style=color:#f92672>|</span> No valid partition     <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34920</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> No valid partition  <span style=color:#f92672>|</span> No valid partition     <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>44920</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> No valid partition  <span style=color:#f92672>|</span> No valid partition     <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34700</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>52</span>           <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>52</span>            <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>100</span>              <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34500</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>48</span>           <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>48</span>            <span style=color:#f92672>|</span> test: <span style=color:#ae81ff>100</span>              <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
<span style=color:#f92672>|</span> <span style=color:#ae81ff>192.168.8.210</span> <span style=color:#f92672>|</span> <span style=color:#ae81ff>34800</span> <span style=color:#f92672>|</span> online <span style=color:#f92672>|</span> <span style=color:#ae81ff>0</span>            <span style=color:#f92672>|</span> No valid partition  <span style=color:#f92672>|</span> No valid partition     <span style=color:#f92672>|</span>
<span style=color:#f92672>------------------------------------------------------------------------------------------------</span>
</code></pre></div><p><img src=https://user-images.githubusercontent.com/56643819/73815823-12387800-4822-11ea-8ee9-f0071d60fd7c.png alt=image></p><p>In the above picture, the five blue icons are the newly added ones. However, since we just add them, they serve no parititons.</p><h3 id=step-3-data-migration>Step 3 Data migration</h3><p>Run command <code>BALANCE DATA</code>: </p><pre><code class=language-ngql data-lang=ngql>nebula&gt; BALANCE DATA
==============
| ID         |
==============
| 1570761786 |
--------------
</code></pre><p>This command will generate a new plan and start a migration process if the partitions are not equally distributed. For a balanced cluster, re-run <code>BALANCE DATA</code> will not cause any new operations.You can check the running progress of the plan by command <code>BALANCE DATA $id</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nebula&gt; BALANCE DATA 1570761786
<span style=color:#f92672>===============================================================================</span>
| balanceId, spaceId:partId, src-&gt;dst                           | status      |
<span style=color:#f92672>===============================================================================</span>
| <span style=color:#f92672>[</span>1570761786, 1:1, 192.168.8.210:34600-&gt;192.168.8.210:44920<span style=color:#f92672>]</span>   | succeeded   |
-------------------------------------------------------------------------------
| <span style=color:#f92672>[</span>1570761786, 1:1, 192.168.8.210:34700-&gt;192.168.8.210:34920<span style=color:#f92672>]</span>   | succeeded   |
-------------------------------------------------------------------------------
| <span style=color:#f92672>[</span>1570761786, 1:1, 192.168.8.210:34500-&gt;192.168.8.210:34800<span style=color:#f92672>]</span>   | succeeded   |
-------------------------------------------------------------------------------
...//We omitted some examples here.
-------------------------------------------------------------------------------
| <span style=color:#f92672>[</span>1570761786, 1:88, 192.168.8.210:34700-&gt;192.168.8.210:35940<span style=color:#f92672>]</span>  | succeeded   |
-------------------------------------------------------------------------------
| Total:189, Succeeded:170, Failed:0, In Progress:19, Invalid:0 | 89.947090%  |
-------------------------------------------------------------------------------
Got <span style=color:#ae81ff>190</span> rows <span style=color:#f92672>(</span>Time spent: 5454/11095 us<span style=color:#f92672>)</span>
</code></pre></div><p><strong><em>Explanations on the returned results:</em></strong></p><ul><li>The first column is a specific task. </li></ul><p>     Take 1570761786, 1:88, 192.168.8.210:34700->192.168.8.210:35940 for example</p><ul><li><strong>1570761786</strong> is the balance ID</li><li><strong>1:88</strong>, 1 is the spaceId (i.e., space <code>test</code>), 88 is the partition id which is now being moved</li><li><strong>192.168.8.210:34700->192.168.8.210:3594</strong>, moving data from the source instance to the destination instance. The useless data on the source instance will be garbage collected after the migration is finished.</li><li>The second column shows the state (result) of the task, there are four states:<ul><li>Succeeded</li><li>Failed</li><li>In progress</li><li>Invalid</li></ul></li></ul><p>The last row is the summary of the tasks. Some partitions are yet to be migrated.</p><h3 id=step-4-if-stop-data-balance-halfway->Step 4 If stop data balance halfway, &mldr;</h3><p><code>BALANCE DATA STOP</code> command will stop the running plan and return this plan ID. If there is no running balance plan, an error is thrown. </p><blockquote><p>Since a balance plan includes several balance (sub)tasks, <code>BALANCE DATA STOP</code> doesn&rsquo;t stop the running tasks, but rather cancel the subsequent tasks. The running tasks will continue until the executions are completed.</p></blockquote><p>You can run <code>BALANCE DATA $id</code> to show the status of a stopped balance plan.</p><p>After all the running (sub)tasks are completed, you can re-run the <code>BALANCE DATA</code> command again to resume the previous balance plan (if applicable). If there are failed tasks in the stopped plan, the plan will retry. Otherwise, if all the tasks are succeed (and e.g., a new machine is added the cluster), a new balance plan will be created and executed.</p><h3 id=step-5-data-migration-is-done>Step 5 Data Migration is Done</h3><p>In some cases, the data migration will take hours or even days. During the migration, <strong>Nebula Graph</strong> online services are not affected. Once migration is done, the progress will show 100%. You can retry <code>BALANCE DATA</code> to fix those failed tasks. If it can&rsquo;t be fixed after several attempts, please contact us at <a href=https://github.com/vesoft-inc/nebula/issues>GitHub</a>.Finally, use the <code>SHOW HOSTS</code> to check the final partition distribution.</p><pre><code>nebula&gt; SHOW HOSTS
================================================================================================
| Ip            | Port  | Status | Leader count | Leader distribution | *Partition distribution* |
================================================================================================
| 192.168.8.210 | 34600 | online | 3            | test: 3             | test: 37               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34900 | online | 0            | test: 0             | test: 38               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 35940 | online | 0            | test: 0             | test: 37               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34920 | online | 0            | test: 0             | test: 38               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 44920 | online | 0            | test: 0             | test: 38               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34700 | online | 35           | test: 35            | test: 37               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34500 | online | 24           | test: 24            | test: 37               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | 34800 | online | 38           | test: 38            | test: 38               |
------------------------------------------------------------------------------------------------
Got 8 rows (Time spent: 5074/6488 us)
</code></pre><p><img src=https://user-images.githubusercontent.com/56643819/73815873-33996400-4822-11ea-8ad4-eb6072faee94.png alt=image></p><p>As you can tell from the <code>Partition pistribution</code> column, The numbers are close to each other (37 or 38 for an instance), and total partition number is 300. But &mldr;</p><h3 id=step-6-balance-leader>Step 6 Balance leader</h3><p>Statement <code>BALANCE DATA</code> only migrates partitions (with the data). But the leader distribution remains unbalanced, which means old hosts are overloaded-working, while the new ones are not fully used. We can re-distribute raft leader using the command <code>BALANCE LEADER</code>.</p><pre><code class=language-ngql data-lang=ngql>nebula&gt; BALANCE LEADER
</code></pre><p>Seconds later, show the results using the statement <code>SHOW HOSTS</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nebula&gt; SHOW HOSTS
<span style=color:#f92672>================================================================================================</span>
| Ip            | Port  | Status | Leader count | *Leader distribution* | Partition distribution |
<span style=color:#f92672>================================================================================================</span>
| 192.168.8.210 | <span style=color:#ae81ff>34600</span> | online | <span style=color:#ae81ff>13</span>           | test: <span style=color:#ae81ff>13</span>            | test: <span style=color:#ae81ff>37</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34900</span> | online | <span style=color:#ae81ff>12</span>           | test: <span style=color:#ae81ff>12</span>            | test: <span style=color:#ae81ff>38</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>35940</span> | online | <span style=color:#ae81ff>12</span>           | test: <span style=color:#ae81ff>12</span>            | test: <span style=color:#ae81ff>37</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34920</span> | online | <span style=color:#ae81ff>12</span>           | test: <span style=color:#ae81ff>12</span>            | test: <span style=color:#ae81ff>38</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>44920</span> | online | <span style=color:#ae81ff>13</span>           | test: <span style=color:#ae81ff>13</span>            | test: <span style=color:#ae81ff>38</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34700</span> | online | <span style=color:#ae81ff>12</span>           | test: <span style=color:#ae81ff>12</span>            | test: <span style=color:#ae81ff>37</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34500</span> | online | <span style=color:#ae81ff>13</span>           | test: <span style=color:#ae81ff>13</span>            | test: <span style=color:#ae81ff>37</span>               |
------------------------------------------------------------------------------------------------
| 192.168.8.210 | <span style=color:#ae81ff>34800</span> | online | <span style=color:#ae81ff>13</span>           | test: <span style=color:#ae81ff>13</span>            | test: <span style=color:#ae81ff>38</span>               |
------------------------------------------------------------------------------------------------
</code></pre></div><p>According to the <code>Leader distribution</code> column, the RAFT leaders are distributed evenly over all the hosts in the cluster.</p><p><img src=https://user-images.githubusercontent.com/56643819/73815914-5166c900-4822-11ea-8c68-1cd32d5cc4fe.png alt=image></p><p>As the above picture indicates, when <code>BALANCE LEADER</code> runs successfully, the number of <em>Leader distribution</em> on the newly added (the blue icon) and the original instances (the black icon) are close to each other (12 or 13 for an instance). Besides, as there are no change to the <code>Partition distribution</code> number, it indicates that <code>balance leader</code> only involves the re-distribution of leaders from instances.</p><h2 id=batch-scale-in>Batch Scale in</h2><p><strong>Nebula Graph</strong> also supports to go offline a host (and scale in the cluster) during service. The command is<code>BALANCE DATA REMOVE $host_list</code>.For example, command <code>BALANCE DATA REMOVE 192.168.0.1:50000,192.168.0.2:50000</code>removes two hosts, i.e. 192.168.0.1:50000，192.168.0.2:50000, from the cluster.</p><blockquote><p>If replica number cannot meet the quorum requirement after the remove (e.g., remote two machines from a three machine cluster), <strong>Nebula Graph</strong> will reject the request and return an error code.</p></blockquote><h2 id=conclusion>Conclusion</h2><p>In this post, we showed how to balance data and balance work load on a raft-cluster. If you have any questions, please leave your comment. Finally we tak a glance of the data migration process of instance <em>192.168.8.210:34600</em>.</p><p><img src=https://user-images.githubusercontent.com/56643819/73818243-e4563200-4827-11ea-815f-178ccdcc19ae.png alt=image></p><p>The red number indicates a change happend after a command is executed.</p><h2 id=appendix>Appendix</h2><p>This is the <a href=https://github.com/vesoft-inc/nebula>GitHub Repo</a> for <strong>Nebula Graph</strong>. Welcome to try nebula. IF you have any problems please send us an <a href=https://github.com/vesoft-inc/nebula/issues>issue</a>.</p><blockquote class=star-ads><span>Do you like this post? If yes, just give me a</span>
<span class="glyphicon glyphicon-star"></span><span>star:</span>
<a href=https://github.com/vesoft-inc/nebula>https://github.com/vesoft-inc/nebula</a></blockquote><ul class=blog-footer><li><img src=/images/tag.png>
<a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/Features">Features</a></li><li class="nebula-share st-btn"><img src=/images/share.png>
<span>Share Post</span><ul class=blog-footer-share-links><li class=st-custom-button data-network=twitter><img alt="twitter-white sharing button" src=https://platform-cdn.sharethis.com/img/twitter-white.svg>
<span class=st-label>Twitter</span></li><li class=st-custom-button data-network=linkedin><img alt="twitter-white sharing button" src=https://platform-cdn.sharethis.com/img/linkedin-white.svg>
<span class=st-label>Linkedin</span></li><li class=st-custom-button data-network=facebook><img alt="twitter-white sharing button" src=https://platform-cdn.sharethis.com/img/facebook-white.svg>
<span class=st-label>Facebook</span></li><li class=st-custom-button data-network=sharethis><img alt="twitter-white sharing button" src=https://platform-cdn.sharethis.com/img/sharethis-white.svg>
<span class=st-label>Others</span></li></ul></li><li onclick="location.href='\/aseURL=vesoft-inc.github.io\/nebula-website\/en\/posts'"><img src=/images/blog.png>
<a href="/aseURL=vesoft-inc.github.io/nebula-website/en/posts">Back to blog home</a></li><li onclick="location.href='https:\/\/nebula-graph.io/en/posts/index.xml'"><img src=/images/rss.png>
<a onclick="window.open('https:\/\/nebula-graph.io/en/posts/index.xml')" href>RSS</a></li></ul><div id=discourse-comments></div></section><div class=single-side-bar><div class=tags-block><h3 class="glyphicon glyphicon-tag">Tags</h3><ul class=blog-tags><li class=col-md-12><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/posts" class=active>All</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/architecture">architecture</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/community">community</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/deployment">deployment</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/dev-log">dev-log</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/features">features</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/graph-database">graph-database</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/graph-query-language">graph-query-language</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/performance">performance</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/release-notes">release-notes</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/system-testing">system-testing</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/tools">tools</a></li><li><a href="/aseURL=vesoft-inc.github.io/nebula-website/en/tags/use-cases">use-cases</a></li></ul></div><div id=blog-carousel class="carousel slide" data-ride=carousel><ol class=carousel-indicators><li data-target=#blog-carousel data-slide-to=0 class=active></li><li data-target=#blog-carousel data-slide-to=1></li></ol><div class=carousel-inner role=listbox><div class="item active" onclick="location.href='\/en\/posts\/clean-stale-data-with-ttl-in-nebula-graph_\/'"><img src=https://user-images.githubusercontent.com/38887077/77512987-09256800-6eaf-11ea-9b41-e8b87ab51fe8.png alt="How Nebula Graph Automatically Cleans Stale Data with TTL" width=220px height=150px><div class=carousel-caption>How Nebula Graph Automatically Cleans Stale Data with TTL</div></div><div class=item onclick="location.href='\/en\/posts\/github-action-automating-project-process\/'"><img src=https://user-images.githubusercontent.com/38887077/76831941-482a3c80-6863-11ea-9795-009eaa50fc0e.png alt="Automating Your Project Processes with Github Actions" width=220px height=150px><div class=carousel-caption>Automating Your Project Processes with Github Actions</div></div></div><a class="left carousel-control" href=#blog-carousel role=button data-slide=prev><span class="glyphicon glyphicon-chevron-left" aria-hidden=true></span><span class=sr-only>Previous</span></a>
<a class="right carousel-control" href=#blog-carousel role=button data-slide=next><span class="glyphicon glyphicon-chevron-right" aria-hidden=true></span><span class=sr-only>Next</span></a></div><div class=blog-anchors><h3>Contents</h3><nav id=TableOfContents><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#intro-to-the-balance-mechanism>Intro to the balance mechanism</a></li><li><a href=#cluster-data-migration>Cluster data migration</a><ul><li><a href=#step-1-prerequisites>Step 1 Prerequisites</a></li><li><a href=#step-2-add-five-new-instances>Step 2 Add five new instances</a></li><li><a href=#step-3-data-migration>Step 3 Data migration</a></li><li><a href=#step-4-if-stop-data-balance-halfway->Step 4 If stop data balance halfway, &mldr;</a></li><li><a href=#step-5-data-migration-is-done>Step 5 Data Migration is Done</a></li><li><a href=#step-6-balance-leader>Step 6 Balance leader</a></li></ul></li><li><a href=#batch-scale-in>Batch Scale in</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#appendix>Appendix</a></li></ul></nav></div><div class=social-share><h3>Share Post</h3><div class=nebula-share-buttons><div class="st-custom-button st-remove-label" data-network=twitter><img alt="twitter-white sharing button" src=https://platform-cdn.sharethis.com/img/twitter-white.svg></div><div class="st-custom-button st-remove-label" data-network=linkedin><img alt="linkedin-white sharing button" src=https://platform-cdn.sharethis.com/img/linkedin-white.svg></div><div class="st-custom-button st-remove-label" data-network=facebook><img alt="facebook-white sharing button" src=https://platform-cdn.sharethis.com/img/facebook-white.svg></div><div class="st-custom-button st-remove-label" data-network=hackernews><img alt="hackernews-white sharing button" src=https://platform-cdn.sharethis.com/img/hackernews-white.svg></div><div class="st-custom-button st-remove-label" data-network=reddit><img alt="reddit-white sharing button" src=https://platform-cdn.sharethis.com/img/reddit-white.svg></div><div class="st-custom-button st-remove-label" data-network=sharethis><img alt="reddit-white sharing button" src=https://platform-cdn.sharethis.com/img/sharethis-white.svg></div></div></div></div></div><img onclick="location.href='#top'" class=go-ahead src=/images/up.png></div><footer class="container-fluid foot-wrap" name=contact_us><div class=container><p align=center class=statement>Nebula Graph will respect your data privacy per our Privacy Policy</p><div class=row><div class="row-content col-lg-4 col-sm-4 col-xs-4"><h3>Community</h3><ul><li><img src=/img/github.png>
<a href=https://github.com/vesoft-inc/nebula target=_blank>GitHub</a></li><li><img src=/img/forum.png>
<a href=https://discuss.nebula-graph.io/ target=_blank>Forum</a></li><li><img src=/img/slack.png>
<a href=https://join.slack.com/t/nebulagraph/shared_invite/enQtNjIzMjQ5MzE2OTQ2LTM0MjY0MWFlODg3ZTNjMjg3YWU5ZGY2NDM5MDhmOGU2OWI5ZWZjZDUwNTExMGIxZTk2ZmQxY2Q2MzM1OWJhMmY# target=_blank>Slack</a></li></ul></div><div class="row-content col-lg-4 col-sm-4 col-xs-4"><h3>Follow Us</h3><ul><li><img src=/img/twitter.png>
<a href=https://twitter.com/NebulaGraph title=Twitter data-toggle=modal target=_blank data-target>Twitter</a></li><li><img src=/img/weibo.png>
<a href=https://weibo.com/nebulagraph title=WeiBo data-toggle=modal target=_blank data-target>WeiBo</a></li><li><img src=/img/linkedin.png>
<a href=https://www.linkedin.com/company/vesoft-nebula-graph/ title=LinkedIn data-toggle=modal target=_blank data-target>LinkedIn</a></li><li><img src=/img/youtube.png>
<a href=https://www.youtube.com/channel/UC73V8q795eSEMxDX4Pvdwmw title=YouTube data-toggle=modal target=_blank data-target>YouTube</a></li><li><img src=/img/facebook.png>
<a href=https://www.facebook.com/NebulaGraph/ title=FaceBook data-toggle=modal target=_blank data-target>FaceBook</a></li><li><img src=/img/gongzhonghao.png>
<a href title="Official Accounts" data-toggle=modal target=_blank data-target=#myGongzhonghaoModal>Official Accounts</a></li></ul></div><div class="contactus row-content col-lg-4 col-sm-4 col-xs-4"><h3>Contact Us</h3><ul><li><img src=/img/iphone.png>
<a href data-toggle=modal target=_blank data-target>(+86) 0571-28120658</a></li><li><img src=/img/email.png>
<a href=mailto:info@vesoft.com data-toggle=modal target=_blank data-target>info@vesoft.com</a></li><li><img src=/img/weixin.png>
<a href data-toggle=modal target=_blank data-target=#myModal>WeChat</a></li></ul></div></div></div><p align=left style=font-size:16px><img src=/images/VEsoft.png style=margin-right:10px> Copyright &copy;2020 VESoft Inc</p><div class="modal fade" id=myGongzhonghaoModal tabindex=-1 role=dialog aria-labelledby=myModalLabel aria-hidden=true><div class=modal-dialog><div class=modal-content><div class="modal-body wechat"><img src=/images/gonggonghaoCODE.jpg></div><div class=modal-footer><button type=button class="btn btn-default" data-dismiss=modal>close</button></div></div></div></div><div class="modal fade" id=myModal tabindex=-1 role=dialog aria-labelledby=myModalLabel aria-hidden=true><div class=modal-dialog><div class=modal-content><div class="modal-body wechat"><img src=/images/wechat.png></div><div class=modal-footer><button type=button class="btn btn-default" data-dismiss=modal>close</button></div></div></div></div></footer></main><div id=J-star-popup class=nebula-star-popup></div><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/jquery.js"></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/bootstrap.min.js"></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/jquery.easing.min.js"></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/jquery.fittext.js"></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/wow.min.js"></script><script type=text/javascript src="https://platform-api.sharethis.com/js/sharethis.js#property=5e7c2cc315c7990012ae38bc&product=inline-share-buttons" async></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/creative.js"></script><script async defer src=https://buttons.github.io/buttons.js></script><script src="/aseURL=vesoft-inc.github.io/nebula-website/js/init.js"></script><script type=text/javascript>DiscourseEmbed={discourseUrl:'https://discuss.nebula-graph.io/',discourseEmbedUrl:"aseURL=vesoft-inc.github.io\/nebula-website\/en\/posts\/nebula-graph-storage-banlancing-data-migration\/"};(function(){var d=document.createElement('script');d.type='text/javascript';d.async=true;d.src=DiscourseEmbed.discourseUrl+'javascripts/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(d);})();</script></body></html>